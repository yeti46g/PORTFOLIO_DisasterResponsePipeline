# Disaster Response Pipeline Project

 - Skills : ETL (SQLALchemy, SQLite database), NLP (nltk), ML (sklearn, multi-output classification, imbalanced data ), visualization (Plotly), web app (Flask, html, css, bootstrap)

### Table of Contents

	1. Project Summary
	2. Installation
	3. Instructions
	4. File Description

### 1. Project Summary

This project focuses on practicing data engineering skills, including building ETL (Extract, Transform, Load) pipeline and ML (Machine Learning) pipeline using libraries such as sqlalchemy, nltk, and sklearn. It also utilizes Plotly to build interactive web-based data visualizations. Additionally, a web app is built to use ML model to classify user inputs, using boostrap and Flask. 

These above mentioned skills are applied to analyze disaster data provided by [Figure Eight](https://appen.com/). This data set contains real messages that were sent during disaster events. The machine learning pipeline is created to categorize these messages so that they can be sent to an appropriate disaster relief agency. The web app is built so that an emergency worker can input a new message and get classification results in several categories (multi-output classification).

Please note that this project's emphasis is on software engineering skills (i.e. creating basic data pipeline and building web app), rather than machine learning modelling. The ML model created here is a baseline model without in-depth tunning. For example, since running the grid search through a lot of hyperparameters on the author's local machine takes too long, most hyperparameters in the parameter grid are muted. Therefore, the resulting model is not intented for high accuracy classification, but for demonstrating the pipeline.

### 2. Installation

Except for the regular data science libraries such as numpy, pandas, and matplotlib, the following libraries need to be installed to run the codes in this repo.

1. sqlachemy
2. ntlk
3. sklearn
4. pickle



### 3. Instructions
Please download or clone the entire repo to a root directory, and then:


"
1. Run the following commands in the **project's root directory** to set up your database and model.

    - To run ETL pipeline that cleans data and stores in database <br>
        `python data/process_data.py data/disaster_messages.csv data/disaster_categories.csv data/DisasterResponse.db`
    - To run ML pipeline that trains classifier and saves <br>
        `python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl`

2. Run the following command in the **project's root directory** to run your web app.<br>
    `python app/run.py`

3. Go to http://0.0.0.0:3001/

"

		--- source: Udacity data science nanodegree project starter code


### 4. File Description

The files in this repo are organized in the following structure (adapted on Udacity data science nanodegree project instructions):

	- app
	| - template
	| |- master.html # main page of the web app
	| |- go.html # classification result page of web app
	|- run.py # Flask file to run app

	- data
	|- disaster_categories.csv # message id and its corresponding categories
	|- disaster_messages.csv # message data
	|- process_data.py # ETL pipeline script to clean raw data and save to SQLite database
	|- DisasterResponse.db # SQLite database that contains the cleaned data 

	- models
	|- train_classifier.py # ML pipeline script to load data from database and train models
	|- *classifier.pkl # trained model saved in a pickle file; Note that this file is not included<br>
			#in this repo because its size exceeds Github uploading limit; it can be generated by<br>
			#running the train_classifier.py script.


### 5. Acknowledgement

This project is part of Udacity Data Science Nanodegree content. The codes in the repo are built on the starter code provided by the course.  

	



